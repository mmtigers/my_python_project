import requests
from requests.adapters import HTTPAdapter  # <--- è¿½åŠ 
from urllib3.util.retry import Retry       # <--- è¿½åŠ 
from bs4 import BeautifulSoup
import sys
import os
import argparse
import re
import traceback
from typing import List, TypedDict

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã¸ã®ãƒ‘ã‚¹è¨­å®š
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import config
# ã€ä¿®æ­£ã€‘commonå»ƒæ­¢ -> coreãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¸ç§»è¡Œ
from core.logger import setup_logging
from core.database import save_log_generic
from core.utils import get_now_iso

# ã€ä¿®æ­£ã€‘çµ±ä¸€ãƒ­ã‚¬ãƒ¼ã‚’ä½¿ç”¨
logger = setup_logging("bicycle_monitor")

# ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®å®šç¾© (Type Hinting)
class ParkingRecord(TypedDict):
    area_name: str
    status_text: str
    waiting_count: int

class BicycleParkingMonitor:
    """
    é§è¼ªå ´ã®å®šæœŸåˆ©ç”¨å¾…æ©ŸçŠ¶æ³ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã€DBã«è¨˜éŒ²ã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚
    """
    
    def __init__(self) -> None:
        # Configã«å®šç¾©ãŒãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆURLã‚’ä½¿ç”¨
        self.url: str = getattr(config, "BICYCLE_PARKING_URL", "https://www.midi-kintetsu.com/mpns/pa/h-itami/teiki/index.php")
        self.table_name: str = getattr(config, "SQLITE_TABLE_BICYCLE", "bicycle_parking_logs")
        self.records: List[ParkingRecord] = []
    
    def _get_session(self) -> requests.Session:
        """
        ãƒªãƒˆãƒ©ã‚¤æˆ¦ç•¥ã‚’è¨­å®šã—ãŸrequestsã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆã—ã¦è¿”ã™ã€‚
        - æ¥ç¶šã‚¨ãƒ©ãƒ¼ã‚„ä¸€æ™‚çš„ãªã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼(5xx)ã«å¯¾ã—ã¦ã€è‡ªå‹•çš„ã«ãƒªãƒˆãƒ©ã‚¤ã‚’è¡Œã†ã€‚
        - Backoff Factor=1 ã«ã‚ˆã‚Šã€ãƒªãƒˆãƒ©ã‚¤é–“éš”ã‚’ç©ºã‘ã¦ã‚µãƒ¼ãƒãƒ¼è² è·ã‚’è€ƒæ…®ã™ã‚‹ã€‚
        """
        session = requests.Session()
        retries = Retry(
            total=3,                # æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°
            backoff_factor=1,       # ãƒªãƒˆãƒ©ã‚¤é–“éš” (1ç§’, 2ç§’, 4ç§’...)
            status_forcelist=[500, 502, 503, 504],  # ãƒªãƒˆãƒ©ã‚¤å¯¾è±¡ã®HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
            allowed_methods=["GET"] # GETãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ã¿å¯¾è±¡
        )
        adapter = HTTPAdapter(max_retries=retries)
        session.mount("https://", adapter)
        session.mount("http://", adapter)
        return session

    def fetch_and_parse(self) -> bool:
        """
        Webã‚µã‚¤ãƒˆã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€self.recordsã«æ ¼ç´ã™ã‚‹ã€‚
        """
        logger.debug(f"Fetching data from: {self.url}")
        
        # ã€ä¿®æ­£ã€‘ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã—ã¦ãƒªãƒˆãƒ©ã‚¤ã‚’è¡Œã† & æ˜ç¤ºçš„ãªClose (Context Manager)
        try:
            with self._get_session() as session:
                res = session.get(self.url, timeout=15)
                res.encoding = res.apparent_encoding
                
                if res.status_code != 200:
                    # HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚¨ãƒ©ãƒ¼ã¯ã‚µãƒ¼ãƒãƒ¼å´ã®å•é¡Œã®å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚WARNINGã¨ã™ã‚‹
                    logger.warning(f"HTTP Error: {res.status_code}")
                    return False
                    
                soup = BeautifulSoup(res.text, 'html.parser')
                
                # ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ ã«ä¾å­˜ã—ãŸã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
                tables = soup.find_all('table')
                if not tables:
                    logger.warning("No tables found on the page.")
                    return False

                self.records = []
                
                # ç‰¹å®šã®ã‚¨ãƒªã‚¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
                target_keywords = ["éˆ´åŸ", "ä¼Šä¸¹", "é˜ªæ€¥"]
                
                for table in tables:
                    rows = table.find_all('tr')
                    for row in rows:
                        cols = row.find_all(['td', 'th'])
                        text_row = [c.get_text(strip=True) for c in cols]
                        
                        if len(text_row) >= 2:
                            area_name = text_row[0]
                            status_text = text_row[1]
                            
                            if any(k in area_name for k in target_keywords):
                                count = 0
                                match = re.search(r'(\d+)äºº', status_text)
                                if match:
                                    count = int(match.group(1))
                                elif "ç©º" in status_text or "â—‹" in status_text:
                                    count = 0
                                
                                self.records.append({
                                    "area_name": area_name,
                                    "status_text": status_text,
                                    "waiting_count": count
                                })
                return True

        except requests.exceptions.RequestException as e:
            # ã€ä¿®æ­£ã€‘ãƒªãƒˆãƒ©ã‚¤å¾Œã‚‚å¤±æ•—ã—ãŸãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ã¯ã€é‡è¦åº¦ãŒä½ã„ãŸã‚WARNINGã«ç•™ã‚ã‚‹
            # ã“ã‚Œã«ã‚ˆã‚Š DiscordErrorHandler (ERRORä»¥ä¸Šã§é€šçŸ¥) ã‚’å›é¿ã™ã‚‹
            logger.warning(f"Network Connection Failed (Retries exhausted): {e}")
            return False

        except Exception as e:
            # ã€ä¿®æ­£ã€‘äºˆæœŸã›ã¬ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ã‚„ãƒ­ã‚¸ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼ã¯å¼•ãç¶šãERRORã§é€šçŸ¥ã™ã‚‹
            logger.error(f"Unexpected Scraping failed: {e}")
            logger.debug(traceback.format_exc())
            return False

    def save_to_db(self) -> None:
        """å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’DBã«ä¿å­˜ã™ã‚‹"""
        if not self.records:
            logger.debug("No records to save.")
            return

        success_count = 0
        cols = ["timestamp", "area_name", "status_text", "waiting_count"]
        
        for r in self.records:
            try:
                vals = (
                    get_now_iso(),
                    r['area_name'],
                    r['status_text'],
                    r['waiting_count']
                )
                # ã€ä¿®æ­£ã€‘core.database.save_log_generic ã‚’ä½¿ç”¨
                if save_log_generic(self.table_name, cols, vals):
                    success_count += 1
            except Exception as e:
                logger.error(f"DBä¿å­˜ã‚¨ãƒ©ãƒ¼ ({r['area_name']}): {e}")

        logger.debug(f"ğŸ’¾ {success_count}/{len(self.records)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="é§è¼ªå ´å¾…æ©ŸçŠ¶æ³ãƒ¢ãƒ‹ã‚¿ãƒ¼")
    parser.add_argument("--save", action="store_true", help="DBã«ä¿å­˜ã™ã‚‹")
    args = parser.parse_args()

    # è‡ªå‹•å®Ÿè¡Œ(cron)ã‚’æƒ³å®šã—ã€printã§ã¯ãªãloggerã‚’ä½¿ç”¨
    logger.debug("ğŸš² --- Bicycle Parking Monitor ---")
    monitor = BicycleParkingMonitor()
    
    is_success = monitor.fetch_and_parse()
    
    if is_success:
        logger.debug(f"âœ… è§£æå®Œäº†: {len(monitor.records)} ä»¶ã®ã‚¨ãƒªã‚¢æƒ…å ±ã‚’å–å¾—")
        
        if monitor.records:
            for r in monitor.records:
                # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ« DEBUG ã§çµæœã‚’å‡ºåŠ›
                logger.debug(f"  - {r['area_name']}: {r['status_text']}")
            
            if args.save:
                monitor.save_to_db()
    else:
        # ã€ä¿®æ­£ã€‘ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°æ¸ˆã¿ã®å¤±æ•—ãªã‚‰ç•°å¸¸çµ‚äº†ã‚³ãƒ¼ãƒ‰ã‚’å‡ºã•ãªã„é‹ç”¨ã«å¤‰æ›´ã™ã‚‹ã‹ã€
        # Schedulerå´ã§WARNINGã‚’æ¤œçŸ¥ã§ããªã„ãŸã‚ã€exit(1)ã¯æ®‹ã™ãŒ
        # SchedulerãŒãƒ­ã‚°ã‚’åãéš›ã€æ¨™æº–ã‚¨ãƒ©ãƒ¼å‡ºåŠ›ãŒWARNINGãªã‚‰é€šçŸ¥ã—ãªã„åˆ¶å¾¡ã¯é›£ã—ã„ãŸã‚
        # ã“ã“ã§ã¯ã€Œæ—¢çŸ¥ã®å¤±æ•—ã€ã¨ã—ã¦æ­£å¸¸çµ‚äº†(0)ã•ã›ã‚‹ã‹ã€exit(1)ã•ã›ã‚‹ã‹ã®åˆ¤æ–­ã€‚
        # ä»Šå›ã¯ã€Œé€šçŸ¥ãƒã‚¤ã‚ºå‰Šæ¸›ã€ãŒä¸»ç›®çš„ãªã®ã§ã€ã‚¹ã‚¯ãƒªãƒ—ãƒˆå†…ã§WARNINGãƒ­ã‚°ã‚’å‡ºã—ãŸä¸Šã§
        # sys.exit(0) ã™ã‚‹ã“ã¨ã§Schedulerã® "Task failed" é€šçŸ¥ã‚‚æŠ‘åˆ¶ã—ã¾ã™ã€‚
        
        logger.warning("âš ï¸ Task finished incompletely due to network/parsing issues.")
        sys.exit(0) # Schedulerã¸ã®é€šçŸ¥ã‚’æŠ‘åˆ¶